[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "CODA para estudios del microbioma Seminario del proyecto METACIRCLE",
    "section": "",
    "text": "Introducción 👋"
  },
  {
    "objectID": "index.html#objetivos-del-seminario",
    "href": "index.html#objetivos-del-seminario",
    "title": "CODA para estudios del microbioma Seminario del proyecto METACIRCLE",
    "section": "Objetivos del seminario 🎯",
    "text": "Objetivos del seminario 🎯\n1.- Discutiremos las diferencias entre trabajar con conteos absolutos (abundancias absolutas) y conteos normalizados (abundancias relativas).\n2.- Indicaremos los principios de CODA.\n3.- Mostraremos el tool kit de CODA de microbioma.\n4.- CODA en acción: ejemplo de análisis de datos composicionales."
  },
  {
    "objectID": "t1.html#inconvenientes-de-compararar-muestras-con-abundancias-relativas",
    "href": "t1.html#inconvenientes-de-compararar-muestras-con-abundancias-relativas",
    "title": "1  Inconvenientes de trabajar con conteos absolutos y conteos normalizados 🤯",
    "section": "1.1  Inconvenientes de compararar muestras con abundancias relativas  😵",
    "text": "1.1  Inconvenientes de compararar muestras con abundancias relativas  😵\n\nSi normalizamos, de manera de que las sumas totales de las filas no importa, entonces las matrices de distancias usuales entre muestras, cambian.\n\nEjemplo\nGeneramos una tabla de abundancias relativas\n\nset.seed(5)\nlibrary(ecodist)\nOTU_1= round(rnorm(4 ,100 ,10) )\nOTU_2= round(rnorm(4 ,100 ,10) )\nOTU_3= round(rnorm(4 ,1000 ,100) )\ndatos= data.frame(OTU_1,OTU_2,OTU_3)\nrownames(datos)=c(\"S1\",\"S2\",\"S3\",\"S4\")\nknitr::kable(datos)\n\n\n\n\n\nOTU_1\nOTU_2\nOTU_3\n\n\n\n\nS1\n92\n117\n971\n\n\nS2\n114\n94\n1014\n\n\nS3\n87\n95\n1123\n\n\nS4\n101\n94\n920\n\n\n\n\n\nCalculamos la distancia Bray-Curtis de esta tabla de abundancias absolutas\n\nknitr::kable(as.matrix(bcdist(datos),ncol=2)) \n\n\n\n\n\nS1\nS2\nS3\nS4\n\n\n\n\nS1\n0.0000000\n0.0366361\n0.0720322\n0.0361656\n\n\nS2\n0.0366361\n0.0000000\n0.0542145\n0.0457852\n\n\nS3\n0.0720322\n0.0542145\n0.0000000\n0.0900826\n\n\nS4\n0.0361656\n0.0457852\n0.0900826\n0.0000000\n\n\n\n\n\nLa distancia más grande es entre S3 y S4 .\nAhora vamos a normalizar la tabla de abundancias absolutas para luego calcular su distancia de Bray-Curtis.\n\ndatos_norm=apply(datos, 1, function (datos) datos/sum(datos)) \ndatos_norm=t(datos_norm)\nknitr::kable(datos_norm) #Conteos normalizados\n\n\n\n\n\nOTU_1\nOTU_2\nOTU_3\n\n\n\n\nS1\n0.0779661\n0.0991525\n0.8228814\n\n\nS2\n0.0932897\n0.0769231\n0.8297872\n\n\nS3\n0.0666667\n0.0727969\n0.8605364\n\n\nS4\n0.0905830\n0.0843049\n0.8251121\n\n\n\n\n\nDistancia Bray-curtis para los datos normalizados:\n\nknitr::kable(as.matrix(bcdist(datos_norm),ncol=2))\n\n\n\n\n\nS1\nS2\nS3\nS4\n\n\n\n\nS1\n0.0000000\n0.0222295\n0.0376550\n0.0148476\n\n\nS2\n0.0222295\n0.0000000\n0.0307492\n0.0073819\n\n\nS3\n0.0376550\n0.0307492\n0.0000000\n0.0354243\n\n\nS4\n0.0148476\n0.0073819\n0.0354243\n0.0000000\n\n\n\n\n\n Con esta transformación, la mayor distancia es entre S1 y S3 .\n\n\nAlgunas soluciones:\n\nSubsampling, pero se pierde precisión.\nUsar solo proporciones, pero se añaden correlaciones espurias \n\n\n\n\nCorrelación espuria (Pearson 1896).\n\nDos o más OTUs estarán correlacionados simplemente porque los datos han sido transformados a una suma constante.\n\n\n\n\n\nIncoherencia subcomposicional\n\n\n\n\n\n\n Si hemos normalizado, los conteos de los microorganismos (abundancias de los OTUs) no son variables independientes ya que están ligados por el valor de la suma.\n\nA continuación os dejamos la referencia al estudio:\n\n\n\nNearing J at al. (2022) Microbiome differential abundance methods produce different results across 38 datasets \nLos autores reportan lo siguiente:\n\nDiferentes métodos (14 tipos de test) que se utilizan para identificar microbios con abundancias diferenciales, han identificado números y conjuntos de ASVs (unidades taxonómicas operativas) significativas, muy diferentes diferentes, y los resultados dependen del pre-procesamiento de los datos.\nHan encontrado que ALDEx2 y ANCOM-II (ambos métodos composicionales) produjeron los resultados más consistentes entre los estudios (38 conjuntos de datos del gen rRNA 16S con dos grupos de muestras)\n\nConsideremos la figura 4 de este estudio:\n\n\n\nEn el eje x se presenta el porcentaje de variantes de secuencia (en escala \\(\\log_{10}\\)) que son significativas (p-valor <0.05 con corrección Benjamini-Hochberg) para cada conjunto de datos y tipo de test.\nPodemos observar que, en general, el test LEFSe (círculos en color amarillo) y los tests limma presentan mayor porcentaje de réplicas donde se observaron diferencias significativas en la abundancia de la ASV bajo estudio en un grupo u otro de los conjuntos de datos estudiados. Est diferencia esmayor con los datos filtrados."
  },
  {
    "objectID": "t1.html#solución",
    "href": "t1.html#solución",
    "title": "1  Inconvenientes de trabajar con conteos absolutos y conteos normalizados 🤯",
    "section": "1.2 Solución",
    "text": "1.2 Solución\n\n\n Utilizamos otra forma de “normalizar” que preserva la composición de cada muestra y nos permite compararlas:\n\nCODA= Compositional Data Analysis"
  },
  {
    "objectID": "t2.html#metodología",
    "href": "t2.html#metodología",
    "title": "2  Principios básicos del CODA",
    "section": "2.1  Metodología ",
    "text": "2.1  Metodología"
  },
  {
    "objectID": "t3.html",
    "href": "t3.html",
    "title": "3  Reemplazos composicionales del tool kit tradicional 👀",
    "section": "",
    "text": "Distancia de Aitchison\n\nLa distancia de Aitchison no es más que la distancia euclidiana entre muestras después de la transformación clr. Específicamente:\n\n\\[d(x_i,x_j) = \\sqrt{ \\sum_{k=1}^{D} \\left( log \\left(\\frac{x_{ik}}{g(\\mathbf{x}_i)}\\right) - log \\left(\\frac{x_{jk}}{g(\\mathbf{x}_j)} \\right) \\right)^2}\n\\]\n\nLa distancia Euclídea: \\[d(y_i,y_j) = \\sqrt{ \\sum_{k=1}^{N} \\left( y_{ik} - y_{jk} \\right)^2}\n\\]\n\nEjemplo de distancia euclídea:\n\n\n\n\nCorrelaciones de datos composicionales\nExisten varias técnicas para analizar la correlación de los datos del microbioma que suelen ser matrices “sparce”. Uno de ellos es r-sparc."
  },
  {
    "objectID": "coda_ejemplo1.html#carga-de-librerías-de-r-que-utilizaremos",
    "href": "coda_ejemplo1.html#carga-de-librerías-de-r-que-utilizaremos",
    "title": "4  CODA en acción 😎:",
    "section": "4.1 Carga de librerías de R que utilizaremos",
    "text": "4.1 Carga de librerías de R que utilizaremos\n\nset.seed(1)\nlibrary(data.table)\nlibrary(compositions) \nlibrary(zCompositions) \nlibrary(ALDEx2) \nlibrary(kableExtra)\nlibrary(ggplot2)\nlibrary(easyCODA)\nlibrary(RColorBrewer)\nlibrary(robCompositions)\nlibrary(dendextend)\nlibrary(coda4microbiome)\nlibrary(propr)\nlibrary(ppclust)\nlibrary(factoextra)\nlibrary(cluster)\nlibrary(fclust)\nlibrary(nnet)\nlibrary(corrplot)\nsource(\"funcionsCODACesc.R\")"
  },
  {
    "objectID": "coda_ejemplo1.html#carga-y-limpieza-de-los-datos",
    "href": "coda_ejemplo1.html#carga-y-limpieza-de-los-datos",
    "title": "4  CODA en acción 😎:",
    "section": "4.2 Carga y limpieza de los datos",
    "text": "4.2 Carga y limpieza de los datos\n\nDF.0=read.table(\"count_table_otus.tsv\",header=TRUE,sep=\"\\t\")\nrownames(DF.0)=DF.0[,1]\nDF.0=DF.0[,-1]\n# Eliminando los _\nrownames(DF.0)=gsub(\"_\",\".\", rownames(DF.0))\ncolnames(DF.0)=gsub(\"_\",\".\", colnames(DF.0))\n\n# Las filas deben ser muestras y las columnas taxa \nDF.0=t(DF.0)\ndim(DF.0)\n\n[1] 218 280\n\n# [1] 218 280\n# Eliminamos las filas y columnas con 0\nDF.0=DF.0[apply(DF.0, 1, sum)>0,apply(DF.0, 2, sum)>0]\nmostres=rownames(DF.0)\nbitxos=colnames(DF.0)\ncolnames(DF.0)=1:dim(DF.0)[2]\nGrups=as.factor(substr(mostres,1,1)) \ncolors=c(\"green\",\"blue\",\"brown\")[Grups]"
  },
  {
    "objectID": "coda_ejemplo1.html#tratamiento-de-los-ceros",
    "href": "coda_ejemplo1.html#tratamiento-de-los-ceros",
    "title": "4  CODA en acción 😎:",
    "section": "4.3 Tratamiento de los ceros",
    "text": "4.3 Tratamiento de los ceros\n\n#Proporciones de ceros por muestras\nZeros.row=apply(DF.0,MARGIN=1,FUN=function(x){length(x[x==0])/length(x)}) \n\n#Proporciones de ceros por taxa\nZeros.col=apply(DF.0,MARGIN=2,FUN=function(x){length(x[x==0])/length(x)}) \n\nhist(Zeros.row,breaks=20,freq=FALSE,xlim=c(0.5,0.9),xlab=\"No. of 0's\", main=\"Proportions of 0's in samples\")\n\n\n\n\n\n\n\nhist(Zeros.col,breaks=20,freq=FALSE,xlab=\"No. of 0's\", main=\"Proportions of 0 at taxa\")\n\n\n\n\n\n\n\n\n\n# zPatterns de la librería zCompositions\nzPatterns(DF.0,label=0,suppress.print=TRUE,main=\"Global\")\n\n\n\n\nZero Patterns in global sample\n\n\n\n\n\nzPatterns(DF.0[Grups==\"A\",],label=0,suppress.print=TRUE,main=\"Global\")\n\n\n\n\nZero Patterns in Adults samples\n\n\n\n\n\nzPatterns(DF.0[Grups==\"E\",],label=0,suppress.print=TRUE,main=\"Global\")\n\n\n\n\nZero Patterns in Ederly samples\n\n\n\n\n\nzPatterns(DF.0[Grups==\"I\",],label=0,suppress.print=TRUE,main=\"Global\")\n\n\n\n\nZero Patterns in Infants samples"
  },
  {
    "objectID": "coda_ejemplo1.html#géneros-que-aparecen-únicamente-en-un-tipo-de-muestra",
    "href": "coda_ejemplo1.html#géneros-que-aparecen-únicamente-en-un-tipo-de-muestra",
    "title": "4  CODA en acción 😎:",
    "section": "4.4 Géneros que aparecen únicamente en un tipo de muestra",
    "text": "4.4 Géneros que aparecen únicamente en un tipo de muestra\n\nSolo en Adultos\n\n\nbitxos.nomesA=bitxos[which(apply(DF.0[Grups!=\"A\",], 2, sum)==0)]\nlength(bitxos.nomesA)\n\n[1] 4\n\nbitxos.nomesA%>%\n  kbl() %>%\n  kable_styling()\n\n\n\n \n  \n    x \n  \n \n\n  \n    OTU.196 \n  \n  \n    OTU.254 \n  \n  \n    OTU.260 \n  \n  \n    OTU.270 \n  \n\n\n\n\n\n\nSolo en los Ancianos (Elderly)\n\n\nbitxos.nomesE=bitxos[which(apply(DF.0[Grups!=\"E\",], 2, sum)==0)]\nlength(bitxos.nomesE)\n\n[1] 16\n\nbitxos.nomesE%>%\n  kbl() %>%\n  kable_styling()\n\n\n\n \n  \n    x \n  \n \n\n  \n    OTU.147 \n  \n  \n    OTU.153 \n  \n  \n    OTU.165 \n  \n  \n    OTU.214 \n  \n  \n    OTU.216 \n  \n  \n    OTU.217 \n  \n  \n    OTU.234 \n  \n  \n    OTU.239 \n  \n  \n    OTU.246 \n  \n  \n    OTU.251 \n  \n  \n    OTU.253 \n  \n  \n    OTU.257 \n  \n  \n    OTU.261 \n  \n  \n    OTU.271 \n  \n  \n    OTU.273 \n  \n  \n    OTU.278 \n  \n\n\n\n\n\n\nSolo en los Infantes\n\n\nbitxos.nomésI=bitxos[which(apply(DF.0[Grups!=\"I\",], 2, sum)==0)]\nlength(bitxos.nomésI)\n\n[1] 6\n\nbitxos.nomésI%>%\n  kbl() %>%\n  kable_styling()\n\n\n\n \n  \n    x \n  \n \n\n  \n    OTU.119 \n  \n  \n    OTU.138 \n  \n  \n    OTU.220 \n  \n  \n    OTU.225 \n  \n  \n    OTU.277 \n  \n  \n    OTU.279 \n  \n\n\n\n\n\n\nSolo en uno\n\n\nNomes.a.un=which(apply(DF.0[Grups!=\"A\",], 2, sum)==0 | apply(DF.0[Grups!=\"E\",], 2, sum)==0 | apply(DF.0[Grups!=\"I\",], 2, sum)==0)"
  },
  {
    "objectID": "coda_ejemplo1.html#imputación-de-ceros-con-la-previa-de-jeffreys",
    "href": "coda_ejemplo1.html#imputación-de-ceros-con-la-previa-de-jeffreys",
    "title": "4  CODA en acción 😎:",
    "section": "4.5 Imputación de ceros con la previa de Jeffreys",
    "text": "4.5 Imputación de ceros con la previa de Jeffreys\nSe asume que los valores no cero siguen una distribución log-normal pero están truncados en cero. Los ceros están presentes debido a un valor subyacente que se encuentra por debajo del umbral de detección. En este caso se ha puesto una previa no informativa. Los parámetros de estiman via MCMC.\nLos ceros se reemplazan con los valores de la distribución predictiva posterior.\n\n#cmultRepl de la librería zCompositions Bayesian-Multiplicative replacement of count zeros\n# previa de Jeffreys 1/2, todos los valores tienen la misma probabilidad de ocurrir.\n\nDF.J=cmultRepl(DF.0, method=\"user\", t=matrix(1/dim(DF.0)[2],nrow=dim(DF.0)[1],ncol=dim(DF.0)[2]),s=rep(dim(DF.0)[2]/2,dim(DF.0)[1]),\n               output=\"p-counts\",suppress.print=TRUE)"
  },
  {
    "objectID": "coda_ejemplo1.html#imputación-de-ceros-con-geometric-bayesian-multiplicative",
    "href": "coda_ejemplo1.html#imputación-de-ceros-con-geometric-bayesian-multiplicative",
    "title": "4  CODA en acción 😎:",
    "section": "4.6 Imputación de ceros con Geometric Bayesian multiplicative",
    "text": "4.6 Imputación de ceros con Geometric Bayesian multiplicative\nSe asume que los valores observados no cero siguen una distribución geométrica (modeliza la probabilidad de que ocurra el primer éxito en una serie de ensayos independientes de Bernoulli, donde un éxito es la observación de un valor no cero). Por lo tanto, los ceros se consideran “fracasos” en esta distribución.\nSe estima el parámetro de la distribución geométrica a partir de los valores observados no cero en el conjunto de datos. Se utiliza una distribución previa para el parámetro de la distribución geométrica. Se obtiene la distribución posterior del parámetro con la que se generan valores imputados para los ceros.\n\n# Este es el método por defecto de cmultRepl \n# Hay que quitar columnas con solo una entrada diferente a 0\nUnics=which(apply(DF.0, 2, function(x){length(which(x>0))})==1)\nbitxos.unics=bitxos[Unics]\nlength(bitxos.unics)\n\n[1] 3\n\nbitxos.unics%>%\n  kbl(col.names =NULL) %>%\n  kable_styling()\n\n\n\n\n  \n    OTU.138 \n  \n  \n    OTU.220 \n  \n  \n    OTU.251 \n  \n\n\n\n\n\nDamos un vistazo a lo que nos perderemos si las quitamos: Sus frecuencias relativas dentro de sus muestras únicas\n\nQuè.ens.perdem=rep(0,length(Unics))\nfor (i in 1:length(Unics)){\n  y=attr(Unics,\"names\")[i]\n  x=which(DF.0[,y]>0)\n  Què.ens.perdem[i]=DF.0[x,y]/sum(DF.0[x,])\n}\nround(Què.ens.perdem,6)\n\n[1] 0.024063 0.001049 0.000288\n\n\nLa matriz con los ceros imputados …\n\nDF.0U=DF.0[,-Unics]\nDF.GBM=cmultRepl(DF.0U,method=\"GBM\",output=\"p-counts\",suppress.print=TRUE)\nbitxos.nounics=bitxos[-Unics]"
  },
  {
    "objectID": "coda_ejemplo1.html#imputación-de-ceros-con-un-método-iterativo",
    "href": "coda_ejemplo1.html#imputación-de-ceros-con-un-método-iterativo",
    "title": "4  CODA en acción 😎:",
    "section": "4.7 Imputación de ceros con un método Iterativo",
    "text": "4.7 Imputación de ceros con un método Iterativo\nEl método EM se basa en la idea de maximizar una función de verosimilitud incompleta, donde se asume que los datos faltantes son valores no observados o “datos ocultos”.\n\n# impRZilr de la librería robCompositions\n# \n# Tarda mucho ... \n# DF.0n=as.data.frame(DF.0)\n# DF.0n=as.data.frame(apply(DF.0n,MARGIN=2,as.numeric))\n# DF.It=impRZilr(DF.0n, eps=0.05, method = \"pls\", dl=rep(1, dim(DF.0)[2]),maxit = 10,verbose = FALSE)\n# saveRDS(DF.It, file=\"DFItnou.RData\")\n\n\nDF.It=readRDS(\"DFItnou.RData\")$x"
  },
  {
    "objectID": "coda_ejemplo1.html#qué-método-para-imputar-los-ceros-es-mejor",
    "href": "coda_ejemplo1.html#qué-método-para-imputar-los-ceros-es-mejor",
    "title": "4  CODA en acción 😎:",
    "section": "4.8 ¿Qué método para imputar los ceros es mejor?",
    "text": "4.8 ¿Qué método para imputar los ceros es mejor?\n(Lubbe-Filznoser-Templ Chemolab 2021)\nComparando matrices de correlaciones de Kynclova-Hron-Filzmoser: un valor pequeño indica que el método es mejor.\nLa función corCoDa del paquete robCompositions que las calcula no aguanta matrices grandes (al menos en el portátil), por lo tanto lo hacemos por muestreo.\n\n# Sustitución de los ceros por algo muy pequeño\n# multRepl de la librería zCompositions\nDF.0.alt=multRepl(DF.0,dl=rep(1, ncol(DF.0)),frac=10^(-12),label=0)\nDF.0.alt.U=DF.0.alt[,-Unics]\n\n\n# X Y con las mismas dimensiones\n# m < 30 o  da NaN\nf=function(X,Y,m){\nx=sample(dim(X)[2],m)\n(1/m)^2*sum((corCoDa(X[,x])-corCoDa(Y[,x]))^2)\n}\nmean(replicate(200,f(DF.0.alt,DF.J,25)))\nmean(replicate(200,f(DF.0.alt.U,DF.GBM,25)))\nmean(replicate(200,f(DF.0.alt,DF.It,25)))\n# [1] 0.01688433 0.01664001\n# [1] 0.009403911 0.009399878\n# [1] 0.06253841 0.05720421"
  },
  {
    "objectID": "coda_ejemplo1.html#comparando-matrices-de-distancias-de-aitchison-valor-pequeño-indica-mejor.",
    "href": "coda_ejemplo1.html#comparando-matrices-de-distancias-de-aitchison-valor-pequeño-indica-mejor.",
    "title": "4  CODA en acción 😎:",
    "section": "4.9 Comparando matrices de distancias de Aitchison: valor pequeño indica mejor.",
    "text": "4.9 Comparando matrices de distancias de Aitchison: valor pequeño indica mejor.\n\n(1/dim(DF.0)[1])^2*sum((aDist(DF.0.alt)-aDist(DF.J))^2)\n\n[1] 20279.85\n\n(1/dim(DF.0U)[1])^2*sum((aDist(DF.0.alt.U)-aDist(DF.GBM))^2)\n\n[1] 19972.44\n\n(1/dim(DF.0)[1])^2*sum((aDist(DF.0.alt)-aDist(DF.It))^2)\n\n[1] 11971.71\n\n\n\nImputa0=c(\"J\",\"GBM\",\"It\")\nImputa0=\"GBM\"\nif (Imputa0==\"J\"){\n    DF=DF.J\n}\nif (Imputa0==\"GBM\"){\n  DF.0=DF.0U\n  DF=DF.GBM\n  bitxos=bitxos.nounics\n} \nif (Imputa0==\"It\"){\n    DF=DF.It\n}"
  },
  {
    "objectID": "coda_ejemplo1.html#si-se-quieren-filtrar-las-muestras-outliers",
    "href": "coda_ejemplo1.html#si-se-quieren-filtrar-las-muestras-outliers",
    "title": "4  CODA en acción 😎:",
    "section": "4.10 Si se quieren filtrar las muestras outliers",
    "text": "4.10 Si se quieren filtrar las muestras outliers\nQuitamos muestras que contribuyen a tener mucha varianza\nUtilizó codaSeq.outlier de funcionsCODACesc.R adaptada de EasyCODA, que le da error\nLa función codaSeq.outlier ayuda a identificar observaciones que se desvían significativamente de la estructura típica de las composiciones.\nEl algoritmo de detección de outliers implementado en codaSeq.outlier se basa en la suposición de que los datos de composición siguen una distribución log-ratio multivariada.\n\nDF.CLR=acomp(DF)\nCSOut=codaSeq.outlier(DF.CLR, plot.me=TRUE)\n\n\n\n\n\n\n\noutliers=CSOut$bad\nbones=CSOut$good\n\nLas muestras outliers son\n\nmostres[outliers]\n\n [1] \"A04T4\" \"A05T1\" \"A07T2\" \"A08T4\" \"A08T6\" \"E01T1\" \"E01T2\" \"E06T4\" \"E09T2\"\n[10] \"E10T3\" \"I01T2\" \"I01T4\" \"I02T7\" \"I05T2\" \"I07T4\" \"I07T6\" \"I07T7\" \"I09T2\"\n[19] \"I10T6\"\n\n\nSi quitamos estas muestras, tenemos que volver a controlar que no nos quede ninguna columna de 0s\n\nDF.0B=DF.0[bones,]\nconserv=which(apply(DF.0B, 2, sum)>0) \nDF.0B=DF.0B[ ,conserv]\n\n\n# si se emplea DF.It, igual conviene re-calcularlo porque depende de las muestras\n\n# Hay que quitar bichos que hayan quedado a 0 en todo\nconserv=which(apply(DF.0B, 2, sum)>0) \nDF.0B=DF.0B[ ,conserv]\nDF.0Bn=as.data.frame(DF.0B)\nDF.0Bn=as.data.frame(apply(DF.0Bn,MARGIN=2,as.numeric))\nDFB.It=impRZilr(DF.0Bn, eps=0.05, method = \"pls\", dl=rep(1, dim(DF.0B)[2]),maxit = 10,verbose = FALSE)\nsaveRDS(DFB.It, file=\"DFItBnou.RData\")\n\nCon la función QuinesMostres indicamos si cogemos solo las muestras buenas o todas\n\n#QuinesMostres=c(\"totes\",\"bones\") \nQuinesMostres=\"bones\"\nif (QuinesMostres==\"bones\"){\n  DF.0=DF.0B\n  DF=DF[bones,conserv]\n  Grups=Grups[bones]\ncolors=colors[bones]\nmostres=mostres[bones]\n    bitxos=bitxos[conserv]\n}\nDF.CLR=acomp(DF)\nDF.prop=t(apply(DF, 1, function(x){x/sum(x)}))"
  },
  {
    "objectID": "coda_ejemplo1.html#sin-filtrar-variables",
    "href": "coda_ejemplo1.html#sin-filtrar-variables",
    "title": "4  CODA en acción 😎:",
    "section": "4.11 Sin filtrar variables",
    "text": "4.11 Sin filtrar variables\nNo hacemos biplot porque con tanta variable no se ve nada\n\n4.11.1 Clustering jerárquico\nA diferencia del algoritmo de Ward estándar (agrupa observaciones similares en función de una medida de distancia entre ellas), donde todas las observaciones tienen el mismo peso, el enfoque ponderado permite considerar la importancia relativa de cada observación en el proceso de agrupamiento.\n\n# Clustering jerárquico con distancias euclidianas con pesos\n# Función WARD de EasyCODA, necesita que la matriz de clr's se calcule con la misma librería\nDF.CLR.W=CLR(DF)\nhc=WARD(DF.CLR.W,weight=TRUE)\ndend=as.dendrogram(hc)\nlabels_colors(dend)=colors[hc$order]\npar(cex=0.75)\nplot(dend, main = \"\")\n\n\n\n\n\n\n\npar(cex=1)\n# No dibujamos barplot de composiciones porque el gráfico no es informativo\n\nMatriz para entender los resultados:\n\nclust1=data.frame(Orig=Grups[hc$order],\n             clust=cutree(dend, k = 3)[order.dendrogram(dend)])\ntable(clust1)%>%\n  kbl() %>%\n  kable_styling()\n\n\n\n \n  \n      \n    1 \n    2 \n    3 \n  \n \n\n  \n    A \n    51 \n    14 \n    0 \n  \n  \n    E \n    25 \n    28 \n    16 \n  \n  \n    I \n    57 \n    0 \n    8"
  },
  {
    "objectID": "coda_ejemplo1.html#aldex",
    "href": "coda_ejemplo1.html#aldex",
    "title": "4  CODA en acción 😎:",
    "section": "4.12 ALDEx",
    "text": "4.12 ALDEx\nPermite identificar características (como especies microbianas o genes) que muestran diferencias significativas entre grupos de muestras (A, E, I) utilizando un enfoque basado en la inferencia bayesiana. Las muestras se extraen de la distribución Dirichlet (la beta en el caso multivariado).\n\nDF0.Aldex=rbind(DF.0[Grups==\"A\",],DF.0[Grups==\"E\",], DF.0[Grups==\"I\",])\nDF0.t=data.frame(t(DF0.Aldex))\nconds=c(rep(\"A\", dim(DF.0[Grups==\"A\",])[1]),rep(\"E\", dim(DF.0[Grups==\"E\",])[1]), rep(\"I\", dim(DF.0[Grups==\"I\",])[1]))\n#'\nx.clr.kw=aldex.clr(DF0.t[,1:5], conds=conds[1:5], mc.samples=10, verbose=FALSE)\nmc.instances <- numMCInstances(x.clr.kw)\nmc.all <- getMonteCarloInstances(x.clr.kw)\n\n\nDF0.Aldex=rbind(DF.0[Grups==\"A\",],DF.0[Grups==\"E\",], DF.0[Grups==\"I\",])\nDF0.t=data.frame(t(DF0.Aldex))\nconds=c(rep(\"A\", dim(DF.0[Grups==\"A\",])[1]),rep(\"E\", dim(DF.0[Grups==\"E\",])[1]), rep(\"I\", dim(DF.0[Grups==\"I\",])[1]))\n#\nx.clr.kw=aldex.clr(DF0.t, conds=conds, mc.samples=1000, verbose=FALSE)\nx.kw=aldex.kw(x.clr.kw, verbose=FALSE)\n# valores esperados del test Kruskal-Wallis y un glm sobre los datos\nmm=model.matrix(~conds,data.frame(conds))\nx.clr.glm=aldex.clr(DF0.t, conds=mm, mc.samples=1000, verbose=FALSE)\nx.glm=aldex.glm(x.clr.glm, mm)\n#\nx.tot=cbind(bitxos,x.kw,x.glm) \nsaveRDS(x.tot, file=\"xtotTotal.RData\")\n\n\nx.tot=readRDS(\"xtotTotal.RData\")\nx.tot$bitxos=paste0(\"OTU.\",c(paste(\"00\",1:9),paste(\"0\",10:99),100:276))\n\n\nx.tot=readRDS(\"xtotTotal.RData\")\n\n\nhead(x.tot)%>%\n  kbl() %>%\n  kable_styling()\n\n\n\n \n  \n    bitxos \n    kw.ep \n    kw.eBH \n    glm.ep \n    glm.eBH \n    Intercept::Est \n    Intercept::SE \n    Intercept::t.val \n    Intercept::pval \n    condsE:Est \n    condsE:SE \n    condsE:t.val \n    condsE:pval \n    condsI:Est \n    condsI:SE \n    condsI:t.val \n    condsI:pval \n    Intercept::pval.holm \n    condsE:pval.holm \n    condsI:pval.holm \n  \n \n\n  \n    OTU.00 1 \n    0.0801547 \n    0.1570136 \n    0.0303225 \n    0.0797589 \n    11.554637 \n    0.1854587 \n    62.30798 \n    0 \n    0.3413483 \n    0.2584491 \n    1.3206900 \n    0.1903230 \n    0.6980545 \n    0.2622782 \n    2.6615681 \n    0.0087659 \n    0 \n    1.0000000 \n    0.9977548 \n  \n  \n    OTU.00 2 \n    0.0206440 \n    0.0579031 \n    0.8077564 \n    0.8585892 \n    10.371632 \n    0.3250578 \n    31.95125 \n    0 \n    -0.2775871 \n    0.4529898 \n    -0.6145031 \n    0.5414810 \n    -0.1943723 \n    0.4597011 \n    -0.4203944 \n    0.6775020 \n    0 \n    1.0000000 \n    1.0000000 \n  \n  \n    OTU.00 3 \n    0.0000000 \n    0.0000001 \n    0.0000000 \n    0.0000001 \n    11.002260 \n    0.2578001 \n    42.77776 \n    0 \n    -1.3076398 \n    0.3592617 \n    -3.6427641 \n    0.0003698 \n    1.0649519 \n    0.3645844 \n    2.9277117 \n    0.0042881 \n    0 \n    0.0972514 \n    0.8239804 \n  \n  \n    OTU.00 4 \n    0.0005343 \n    0.0031584 \n    0.0136801 \n    0.0424850 \n    8.736565 \n    0.5371141 \n    16.30443 \n    0 \n    -2.1272875 \n    0.7485045 \n    -2.8451954 \n    0.0060917 \n    -0.4423784 \n    0.7595941 \n    -0.5828832 \n    0.5672890 \n    0 \n    0.8664487 \n    1.0000000 \n  \n  \n    OTU.00 5 \n    0.0017423 \n    0.0078298 \n    0.0006925 \n    0.0041580 \n    12.369761 \n    0.1919890 \n    64.43325 \n    0 \n    -0.8996720 \n    0.2675496 \n    -3.3628596 \n    0.0009802 \n    -0.0119042 \n    0.2715135 \n    -0.0438439 \n    0.9315986 \n    0 \n    0.2511886 \n    1.0000000 \n  \n  \n    OTU.00 6 \n    0.0008415 \n    0.0044561 \n    0.0350900 \n    0.0835593 \n    9.693963 \n    0.2458796 \n    39.53140 \n    0 \n    0.0115472 \n    0.3426497 \n    0.0338475 \n    0.9425424 \n    0.8127551 \n    0.3477263 \n    2.3506040 \n    0.0246736 \n    0 \n    1.0000000 \n    1.0000000 \n  \n\n\n\n\n\nAquí podemos basarnos en:\n\nkw.eBH: ajuste de p-valores utilizando el método de Benjamini-Hochberg (BH) después de realizar un análisis de varianza de Kruskal-Wallis (KW).\nglm.eBH: ajuste de p-valores utilizando el método de Benjamini-Hochberg (BH) después de ajustar un modelo lineal generalizado (GLM)\nmodel.condsE: se refiere al modelo de regresión ajustado utilizando las variables de composición como predictores y la condición “condsE” como la variable de respuesta.\n\n\nsignif.p1=which(x.tot$kw.eBH < 0.01)\nsignif.g1=which(x.tot$glm.eBH < 0.01)\nsignif.p1=intersect(signif.p1,signif.g1)\n\nlength(signif.p1)\n\n[1] 40\n\nx.tot.sign=x.tot[signif.p1,c(3,5,19,20)]\nnames(x.tot.sign)=c(\"p-val KW corregit\", \"p-val glm corregit\", \"p-val E vs A corregit\", \"p-val I vs A corregit\")\nrownames(x.tot.sign)=bitxos[signif.p1]\n\nx.tot.sign%>%\n  kbl() %>%\n  kable_styling()\n\n\n\n \n  \n      \n    p-val KW corregit \n    p-val glm corregit \n    p-val E vs A corregit \n    p-val I vs A corregit \n  \n \n\n  \n    OTU.003 \n    0.0000001 \n    0.0000001 \n    0.0972514 \n    0.8239804 \n  \n  \n    OTU.005 \n    0.0078298 \n    0.0041580 \n    0.2511886 \n    1.0000000 \n  \n  \n    OTU.008 \n    0.0000070 \n    0.0000016 \n    1.0000000 \n    0.0000085 \n  \n  \n    OTU.011 \n    0.0010125 \n    0.0059162 \n    1.0000000 \n    0.0787355 \n  \n  \n    OTU.013 \n    0.0000433 \n    0.0000009 \n    0.2828991 \n    0.5834326 \n  \n  \n    OTU.016 \n    0.0038849 \n    0.0006962 \n    0.0327502 \n    1.0000000 \n  \n  \n    OTU.020 \n    0.0097656 \n    0.0033890 \n    1.0000000 \n    0.0918489 \n  \n  \n    OTU.022 \n    0.0000326 \n    0.0000097 \n    1.0000000 \n    0.0358061 \n  \n  \n    OTU.025 \n    0.0002163 \n    0.0001919 \n    1.0000000 \n    0.0091624 \n  \n  \n    OTU.030 \n    0.0000002 \n    0.0000000 \n    0.0002360 \n    0.0000002 \n  \n  \n    OTU.034 \n    0.0000000 \n    0.0000000 \n    1.0000000 \n    0.0000011 \n  \n  \n    OTU.038 \n    0.0001000 \n    0.0006898 \n    1.0000000 \n    0.0438187 \n  \n  \n    OTU.044 \n    0.0000015 \n    0.0000002 \n    1.0000000 \n    0.0000073 \n  \n  \n    OTU.046 \n    0.0011419 \n    0.0006191 \n    0.5639068 \n    0.0063995 \n  \n  \n    OTU.047 \n    0.0001250 \n    0.0001823 \n    0.2585800 \n    0.9991464 \n  \n  \n    OTU.049 \n    0.0053303 \n    0.0072328 \n    0.1026868 \n    0.9789205 \n  \n  \n    OTU.050 \n    0.0000264 \n    0.0002277 \n    0.3148470 \n    1.0000000 \n  \n  \n    OTU.052 \n    0.0001237 \n    0.0075423 \n    0.9952454 \n    1.0000000 \n  \n  \n    OTU.054 \n    0.0013229 \n    0.0097693 \n    1.0000000 \n    0.7449599 \n  \n  \n    OTU.057 \n    0.0000489 \n    0.0000655 \n    1.0000000 \n    0.1825672 \n  \n  \n    OTU.059 \n    0.0000000 \n    0.0000000 \n    1.0000000 \n    0.0000004 \n  \n  \n    OTU.060 \n    0.0002872 \n    0.0042465 \n    0.6414395 \n    1.0000000 \n  \n  \n    OTU.063 \n    0.0004161 \n    0.0014406 \n    0.4197127 \n    1.0000000 \n  \n  \n    OTU.064 \n    0.0000174 \n    0.0001876 \n    1.0000000 \n    0.0845730 \n  \n  \n    OTU.066 \n    0.0000025 \n    0.0000036 \n    0.0061348 \n    0.0000254 \n  \n  \n    OTU.067 \n    0.0003092 \n    0.0026454 \n    0.7734519 \n    0.0414771 \n  \n  \n    OTU.075 \n    0.0091920 \n    0.0052941 \n    0.1359825 \n    1.0000000 \n  \n  \n    OTU.077 \n    0.0000837 \n    0.0003245 \n    0.9095772 \n    0.9990152 \n  \n  \n    OTU.078 \n    0.0001277 \n    0.0005558 \n    0.7500225 \n    1.0000000 \n  \n  \n    OTU.086 \n    0.0005582 \n    0.0022955 \n    0.1265078 \n    1.0000000 \n  \n  \n    OTU.088 \n    0.0000000 \n    0.0000000 \n    1.0000000 \n    0.0000000 \n  \n  \n    OTU.091 \n    0.0000012 \n    0.0000026 \n    0.6333928 \n    0.4631056 \n  \n  \n    OTU.096 \n    0.0007613 \n    0.0024963 \n    0.7773785 \n    0.9987909 \n  \n  \n    OTU.100 \n    0.0087198 \n    0.0049639 \n    0.4523354 \n    1.0000000 \n  \n  \n    OTU.101 \n    0.0000000 \n    0.0000000 \n    1.0000000 \n    0.0000138 \n  \n  \n    OTU.102 \n    0.0000000 \n    0.0000053 \n    0.9922000 \n    0.0525332 \n  \n  \n    OTU.111 \n    0.0004223 \n    0.0016563 \n    0.3737696 \n    0.0432119 \n  \n  \n    OTU.113 \n    0.0021340 \n    0.0040426 \n    0.1122496 \n    1.0000000 \n  \n  \n    OTU.118 \n    0.0000004 \n    0.0000050 \n    0.8946424 \n    0.0804560 \n  \n  \n    OTU.154 \n    0.0010999 \n    0.0060971 \n    0.1554282 \n    0.9981464 \n  \n\n\n\n\nprova=unlist(sapply(rownames(x.tot.sign),FUN=function(x) which(x==x.tot$bitxos)))\nnames(prova)=NULL\n\n Si nos restringimos a estos bichos :\n\nDF.s=DF[,prova]\nDF.CLR=acomp(DF.s) \n\n\npcx=princomp(DF.CLR,cor=TRUE) \n\n\ncoloredBiplot(pcx, cex=0.5, col=\"red\",  arrow.len=0, scale=1,var.axes=TRUE,\n    xlab=paste(\"PC1\", round(pcx$sdev[1]^2 / sum(pcx$sdev^2),3), sep=\": \"),\n    ylab=paste(\"PC2\", round(pcx$sdev[2]^2 / sum(pcx$sdev^2),3), sep=\": \"),\n    xlabs.col=colors, main=\"Form biplot\")\n\n\n\n\n\n\n\n\n\nDF.CLR.W.s=CLR(DF.s)\nhc2=WARD(DF.CLR.W.s,weight=TRUE)\ndend.2=as.dendrogram(hc2)\nlabels_colors(dend.2)=colors[hc2$order]\nDFOr=DF.s[hc2$order,]\n#Reordemanos las muestras para dibujar los barplot en el mismo orden\nDFOr.CLR=acomp(DFOr)\nd.names=colnames(DF.s)[order(apply(DF.s, 2, sum), decreasing=T) ]\nnb.cols=dim(DF.s)[2]\ncolors.OTU=colorRampPalette(brewer.pal(length(d.names),\"Spectral\"))(nb.cols)\n#Dibujo\nlayout(matrix(c(1,3,2,3),2,2, byrow=T), widths=c(6,2), height=c(4,4))\npar(mar=c(2,1,1,1)+0.1,cex=0.75)\nplot(dend.2, main = \"\")\nbarplot(DFOr.CLR, legend.text=F, col=colors.OTU, axisnames=F, border=NA, xpd=T,)\npar(mar=c(0,1,1,1)+0.1,cex=1)\nplot(1,2, pch = 1, lty = 1, ylim=c(-20,20), type = \"n\", axes = FALSE, ann = FALSE)\nlegend(x=\"center\", legend=d.names, col=colors.OTU, lwd=5, cex=.6, border=NULL)"
  }
]