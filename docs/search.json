[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "CODA para estudios del microbioma Seminario del proyecto METACIRCLE",
    "section": "",
    "text": "Introducci√≥n üëã"
  },
  {
    "objectID": "index.html#objetivos-del-seminario",
    "href": "index.html#objetivos-del-seminario",
    "title": "CODA para estudios del microbioma Seminario del proyecto METACIRCLE",
    "section": "Objetivos del seminario üéØ",
    "text": "Objetivos del seminario üéØ\n1.- Discutiremos las diferencias entre trabajar con conteos absolutos (abundancias absolutas) y conteos normalizados (abundancias relativas).\n2.- Indicaremos los principios de CODA.\n3.- Mostraremos el tool kit de CODA de microbioma.\n4.- CODA en acci√≥n: ejemplo de an√°lisis de datos composicionales."
  },
  {
    "objectID": "t1.html#inconvenientes-de-compararar-muestras-con-abundancias-relativas",
    "href": "t1.html#inconvenientes-de-compararar-muestras-con-abundancias-relativas",
    "title": "1¬† Inconvenientes de trabajar con conteos absolutos y conteos normalizados ü§Ø",
    "section": "1.1  Inconvenientes de compararar muestras con abundancias relativas  üòµ",
    "text": "1.1  Inconvenientes de compararar muestras con abundancias relativas  üòµ\n\nSi normalizamos, de manera de que las sumas totales de las filas no importa, entonces las matrices de distancias usuales entre muestras, cambian.\n\nEjemplo\nGeneramos una tabla de abundancias relativas\n\nset.seed(5)\nlibrary(ecodist)\nOTU_1= round(rnorm(4 ,100 ,10) )\nOTU_2= round(rnorm(4 ,100 ,10) )\nOTU_3= round(rnorm(4 ,1000 ,100) )\ndatos= data.frame(OTU_1,OTU_2,OTU_3)\nrownames(datos)=c(\"S1\",\"S2\",\"S3\",\"S4\")\nknitr::kable(datos)\n\n\n\n\n\nOTU_1\nOTU_2\nOTU_3\n\n\n\n\nS1\n92\n117\n971\n\n\nS2\n114\n94\n1014\n\n\nS3\n87\n95\n1123\n\n\nS4\n101\n94\n920\n\n\n\n\n\nCalculamos la distancia Bray-Curtis de esta tabla de abundancias absolutas\n\nknitr::kable(as.matrix(bcdist(datos),ncol=2)) \n\n\n\n\n\nS1\nS2\nS3\nS4\n\n\n\n\nS1\n0.0000000\n0.0366361\n0.0720322\n0.0361656\n\n\nS2\n0.0366361\n0.0000000\n0.0542145\n0.0457852\n\n\nS3\n0.0720322\n0.0542145\n0.0000000\n0.0900826\n\n\nS4\n0.0361656\n0.0457852\n0.0900826\n0.0000000\n\n\n\n\n\nLa distancia m√°s grande es entre S3 y S4 .\nAhora vamos a normalizar la tabla de abundancias absolutas para luego calcular su distancia de Bray-Curtis.\n\ndatos_norm=apply(datos, 1, function (datos) datos/sum(datos)) \ndatos_norm=t(datos_norm)\nknitr::kable(datos_norm) #Conteos normalizados\n\n\n\n\n\nOTU_1\nOTU_2\nOTU_3\n\n\n\n\nS1\n0.0779661\n0.0991525\n0.8228814\n\n\nS2\n0.0932897\n0.0769231\n0.8297872\n\n\nS3\n0.0666667\n0.0727969\n0.8605364\n\n\nS4\n0.0905830\n0.0843049\n0.8251121\n\n\n\n\n\nDistancia Bray-curtis para los datos normalizados:\n\nknitr::kable(as.matrix(bcdist(datos_norm),ncol=2))\n\n\n\n\n\nS1\nS2\nS3\nS4\n\n\n\n\nS1\n0.0000000\n0.0222295\n0.0376550\n0.0148476\n\n\nS2\n0.0222295\n0.0000000\n0.0307492\n0.0073819\n\n\nS3\n0.0376550\n0.0307492\n0.0000000\n0.0354243\n\n\nS4\n0.0148476\n0.0073819\n0.0354243\n0.0000000\n\n\n\n\n\n Con esta transformaci√≥n, la mayor distancia es entre S1 y S3 .\n\n\nAlgunas soluciones:\n\nSubsampling, pero se pierde precisi√≥n.\nUsar solo proporciones, pero se a√±aden correlaciones espurias \n\n\n\n\nCorrelaci√≥n espuria (Pearson 1896).\n\nDos o m√°s OTUs estar√°n correlacionados simplemente porque los datos han sido transformados a una suma constante.\n\n\n\n\n\nIncoherencia subcomposicional\n\n\n\n\n\n\n Si hemos normalizado, los conteos de los microorganismos (abundancias de los OTUs) no son variables independientes ya que est√°n ligados por el valor de la suma.\n\nA continuaci√≥n os dejamos la referencia al estudio:\n\n\n\nNearing J at al.¬†(2022) Microbiome differential abundance methods produce different results across 38 datasets \nLos autores reportan lo siguiente:\n\nDiferentes m√©todos (14 tipos de test) que se utilizan para identificar microbios con abundancias diferenciales, han identificado n√∫meros y conjuntos de ASVs (unidades taxon√≥micas operativas) significativas, muy diferentes diferentes, y los resultados dependen del pre-procesamiento de los datos.\nHan encontrado que ALDEx2 y ANCOM-II (ambos m√©todos composicionales) produjeron los resultados m√°s consistentes entre los estudios (38 conjuntos de datos del gen rRNA 16S con dos grupos de muestras)\n\nConsideremos la figura 4 de este estudio:\n\n\n\nEn el eje x se presenta el porcentaje de variantes de secuencia (en escala \\(\\log_{10}\\)) que son significativas (p-valor <0.05 con correcci√≥n Benjamini-Hochberg) para cada conjunto de datos y tipo de test.\nPodemos observar que, en general, el test LEFSe (c√≠rculos en color amarillo) y los tests limma presentan mayor porcentaje de r√©plicas donde se observaron diferencias significativas en la abundancia de la ASV bajo estudio en un grupo u otro de los conjuntos de datos estudiados. Est diferencia esmayor con los datos filtrados."
  },
  {
    "objectID": "t1.html#soluci√≥n",
    "href": "t1.html#soluci√≥n",
    "title": "1¬† Inconvenientes de trabajar con conteos absolutos y conteos normalizados ü§Ø",
    "section": "1.2 Soluci√≥n",
    "text": "1.2 Soluci√≥n\n\n\n Utilizamos otra forma de ‚Äúnormalizar‚Äù que preserva la composici√≥n de cada muestra y nos permite compararlas:\n\nCODA= Compositional Data Analysis"
  },
  {
    "objectID": "t2.html#metodolog√≠a",
    "href": "t2.html#metodolog√≠a",
    "title": "2¬† Principios b√°sicos del CODA",
    "section": "2.1  Metodolog√≠a ",
    "text": "2.1  Metodolog√≠a"
  },
  {
    "objectID": "t3.html",
    "href": "t3.html",
    "title": "3¬† Reemplazos composicionales del tool kit tradicional üëÄ",
    "section": "",
    "text": "Distancia de Aitchison\n\nLa distancia de Aitchison no es m√°s que la distancia euclidiana entre muestras despu√©s de la transformaci√≥n clr. Espec√≠ficamente:\n\n\\[d(x_i,x_j) = \\sqrt{ \\sum_{k=1}^{D} \\left( log \\left(\\frac{x_{ik}}{g(\\mathbf{x}_i)}\\right) - log \\left(\\frac{x_{jk}}{g(\\mathbf{x}_j)} \\right) \\right)^2}\n\\]\n\nLa distancia Eucl√≠dea: \\[d(y_i,y_j) = \\sqrt{ \\sum_{k=1}^{N} \\left( y_{ik} - y_{jk} \\right)^2}\n\\]\n\nEjemplo de distancia eucl√≠dea:\n\n\n\n\nCorrelaciones de datos composicionales\nExisten varias t√©cnicas para analizar la correlaci√≥n de los datos del microbioma que suelen ser matrices ‚Äúsparce‚Äù. Uno de ellos es r-sparc."
  },
  {
    "objectID": "coda_ejemplo1.html#carga-de-librer√≠as-de-r-que-utilizaremos",
    "href": "coda_ejemplo1.html#carga-de-librer√≠as-de-r-que-utilizaremos",
    "title": "4¬† CODA en acci√≥n üòé:",
    "section": "4.1 Carga de librer√≠as de R que utilizaremos",
    "text": "4.1 Carga de librer√≠as de R que utilizaremos\n\nset.seed(1)\nlibrary(data.table)\nlibrary(compositions) \nlibrary(zCompositions) \nlibrary(ALDEx2) \nlibrary(kableExtra)\nlibrary(ggplot2)\nlibrary(easyCODA)\nlibrary(RColorBrewer)\nlibrary(robCompositions)\nlibrary(dendextend)\nlibrary(coda4microbiome)\nlibrary(propr)\nlibrary(ppclust)\nlibrary(factoextra)\nlibrary(cluster)\nlibrary(fclust)\nlibrary(nnet)\nlibrary(corrplot)\nsource(\"funcionsCODACesc.R\")"
  },
  {
    "objectID": "coda_ejemplo1.html#carga-y-limpieza-de-los-datos",
    "href": "coda_ejemplo1.html#carga-y-limpieza-de-los-datos",
    "title": "4¬† CODA en acci√≥n üòé:",
    "section": "4.2 Carga y limpieza de los datos",
    "text": "4.2 Carga y limpieza de los datos\n\nDF.0=read.table(\"count_table_otus.tsv\",header=TRUE,sep=\"\\t\")\nrownames(DF.0)=DF.0[,1]\nDF.0=DF.0[,-1]\n# Eliminando los _\nrownames(DF.0)=gsub(\"_\",\".\", rownames(DF.0))\ncolnames(DF.0)=gsub(\"_\",\".\", colnames(DF.0))\n\n# Las filas deben ser muestras y las columnas taxa \nDF.0=t(DF.0)\ndim(DF.0)\n\n[1] 218 280\n\n# [1] 218 280\n# Eliminamos las filas y columnas con 0\nDF.0=DF.0[apply(DF.0, 1, sum)>0,apply(DF.0, 2, sum)>0]\nmostres=rownames(DF.0)\nbitxos=colnames(DF.0)\ncolnames(DF.0)=1:dim(DF.0)[2]\nGrups=as.factor(substr(mostres,1,1)) \ncolors=c(\"green\",\"blue\",\"brown\")[Grups]"
  },
  {
    "objectID": "coda_ejemplo1.html#tratamiento-de-los-ceros",
    "href": "coda_ejemplo1.html#tratamiento-de-los-ceros",
    "title": "4¬† CODA en acci√≥n üòé:",
    "section": "4.3 Tratamiento de los ceros",
    "text": "4.3 Tratamiento de los ceros\n\n#Proporciones de ceros por muestras\nZeros.row=apply(DF.0,MARGIN=1,FUN=function(x){length(x[x==0])/length(x)}) \n\n#Proporciones de ceros por taxa\nZeros.col=apply(DF.0,MARGIN=2,FUN=function(x){length(x[x==0])/length(x)}) \n\nhist(Zeros.row,breaks=20,freq=FALSE,xlim=c(0.5,0.9),xlab=\"No. of 0's\", main=\"Proportions of 0's in samples\")\n\n\n\n\n\n\n\nhist(Zeros.col,breaks=20,freq=FALSE,xlab=\"No. of 0's\", main=\"Proportions of 0 at taxa\")\n\n\n\n\n\n\n\n\n\n# zPatterns de la librer√≠a zCompositions\nzPatterns(DF.0,label=0,suppress.print=TRUE,main=\"Global\")\n\n\n\n\nZero Patterns in global sample\n\n\n\n\n\nzPatterns(DF.0[Grups==\"A\",],label=0,suppress.print=TRUE,main=\"Global\")\n\n\n\n\nZero Patterns in Adults samples\n\n\n\n\n\nzPatterns(DF.0[Grups==\"E\",],label=0,suppress.print=TRUE,main=\"Global\")\n\n\n\n\nZero Patterns in Ederly samples\n\n\n\n\n\nzPatterns(DF.0[Grups==\"I\",],label=0,suppress.print=TRUE,main=\"Global\")\n\n\n\n\nZero Patterns in Infants samples"
  },
  {
    "objectID": "coda_ejemplo1.html#g√©neros-que-aparecen-√∫nicamente-en-un-tipo-de-muestra",
    "href": "coda_ejemplo1.html#g√©neros-que-aparecen-√∫nicamente-en-un-tipo-de-muestra",
    "title": "4¬† CODA en acci√≥n üòé:",
    "section": "4.4 G√©neros que aparecen √∫nicamente en un tipo de muestra",
    "text": "4.4 G√©neros que aparecen √∫nicamente en un tipo de muestra\n\nSolo en Adultos\n\n\nbitxos.nomesA=bitxos[which(apply(DF.0[Grups!=\"A\",], 2, sum)==0)]\nlength(bitxos.nomesA)\n\n[1] 4\n\nbitxos.nomesA%>%\n  kbl() %>%\n  kable_styling()\n\n\n\n \n  \n    x \n  \n \n\n  \n    OTU.196 \n  \n  \n    OTU.254 \n  \n  \n    OTU.260 \n  \n  \n    OTU.270 \n  \n\n\n\n\n\n\nSolo en los Ancianos (Elderly)\n\n\nbitxos.nomesE=bitxos[which(apply(DF.0[Grups!=\"E\",], 2, sum)==0)]\nlength(bitxos.nomesE)\n\n[1] 16\n\nbitxos.nomesE%>%\n  kbl() %>%\n  kable_styling()\n\n\n\n \n  \n    x \n  \n \n\n  \n    OTU.147 \n  \n  \n    OTU.153 \n  \n  \n    OTU.165 \n  \n  \n    OTU.214 \n  \n  \n    OTU.216 \n  \n  \n    OTU.217 \n  \n  \n    OTU.234 \n  \n  \n    OTU.239 \n  \n  \n    OTU.246 \n  \n  \n    OTU.251 \n  \n  \n    OTU.253 \n  \n  \n    OTU.257 \n  \n  \n    OTU.261 \n  \n  \n    OTU.271 \n  \n  \n    OTU.273 \n  \n  \n    OTU.278 \n  \n\n\n\n\n\n\nSolo en los Infantes\n\n\nbitxos.nom√©sI=bitxos[which(apply(DF.0[Grups!=\"I\",], 2, sum)==0)]\nlength(bitxos.nom√©sI)\n\n[1] 6\n\nbitxos.nom√©sI%>%\n  kbl() %>%\n  kable_styling()\n\n\n\n \n  \n    x \n  \n \n\n  \n    OTU.119 \n  \n  \n    OTU.138 \n  \n  \n    OTU.220 \n  \n  \n    OTU.225 \n  \n  \n    OTU.277 \n  \n  \n    OTU.279 \n  \n\n\n\n\n\n\nSolo en uno\n\n\nNomes.a.un=which(apply(DF.0[Grups!=\"A\",], 2, sum)==0 | apply(DF.0[Grups!=\"E\",], 2, sum)==0 | apply(DF.0[Grups!=\"I\",], 2, sum)==0)"
  },
  {
    "objectID": "coda_ejemplo1.html#imputaci√≥n-de-ceros-con-la-previa-de-jeffreys",
    "href": "coda_ejemplo1.html#imputaci√≥n-de-ceros-con-la-previa-de-jeffreys",
    "title": "4¬† CODA en acci√≥n üòé:",
    "section": "4.5 Imputaci√≥n de ceros con la previa de Jeffreys",
    "text": "4.5 Imputaci√≥n de ceros con la previa de Jeffreys\nSe asume que los valores no cero siguen una distribuci√≥n log-normal pero est√°n truncados en cero. Los ceros est√°n presentes debido a un valor subyacente que se encuentra por debajo del umbral de detecci√≥n. En este caso se ha puesto una previa no informativa. Los par√°metros de estiman via MCMC.\nLos ceros se reemplazan con los valores de la distribuci√≥n predictiva posterior.\n\n#cmultRepl de la librer√≠a zCompositions Bayesian-Multiplicative replacement of count zeros\n# previa de Jeffreys 1/2, todos los valores tienen la misma probabilidad de ocurrir.\n\nDF.J=cmultRepl(DF.0, method=\"user\", t=matrix(1/dim(DF.0)[2],nrow=dim(DF.0)[1],ncol=dim(DF.0)[2]),s=rep(dim(DF.0)[2]/2,dim(DF.0)[1]),\n               output=\"p-counts\",suppress.print=TRUE)"
  },
  {
    "objectID": "coda_ejemplo1.html#imputaci√≥n-de-ceros-con-geometric-bayesian-multiplicative",
    "href": "coda_ejemplo1.html#imputaci√≥n-de-ceros-con-geometric-bayesian-multiplicative",
    "title": "4¬† CODA en acci√≥n üòé:",
    "section": "4.6 Imputaci√≥n de ceros con Geometric Bayesian multiplicative",
    "text": "4.6 Imputaci√≥n de ceros con Geometric Bayesian multiplicative\nSe asume que los valores observados no cero siguen una distribuci√≥n geom√©trica (modeliza la probabilidad de que ocurra el primer √©xito en una serie de ensayos independientes de Bernoulli, donde un √©xito es la observaci√≥n de un valor no cero). Por lo tanto, los ceros se consideran ‚Äúfracasos‚Äù en esta distribuci√≥n.\nSe estima el par√°metro de la distribuci√≥n geom√©trica a partir de los valores observados no cero en el conjunto de datos. Se utiliza una distribuci√≥n previa para el par√°metro de la distribuci√≥n geom√©trica. Se obtiene la distribuci√≥n posterior del par√°metro con la que se generan valores imputados para los ceros.\n\n# Este es el m√©todo por defecto de cmultRepl \n# Hay que quitar columnas con solo una entrada diferente a 0\nUnics=which(apply(DF.0, 2, function(x){length(which(x>0))})==1)\nbitxos.unics=bitxos[Unics]\nlength(bitxos.unics)\n\n[1] 3\n\nbitxos.unics%>%\n  kbl(col.names =NULL) %>%\n  kable_styling()\n\n\n\n\n  \n    OTU.138 \n  \n  \n    OTU.220 \n  \n  \n    OTU.251 \n  \n\n\n\n\n\nDamos un vistazo a lo que nos perderemos si las quitamos: Sus frecuencias relativas dentro de sus muestras √∫nicas\n\nQu√®.ens.perdem=rep(0,length(Unics))\nfor (i in 1:length(Unics)){\n  y=attr(Unics,\"names\")[i]\n  x=which(DF.0[,y]>0)\n  Qu√®.ens.perdem[i]=DF.0[x,y]/sum(DF.0[x,])\n}\nround(Qu√®.ens.perdem,6)\n\n[1] 0.024063 0.001049 0.000288\n\n\nLa matriz con los ceros imputados ‚Ä¶\n\nDF.0U=DF.0[,-Unics]\nDF.GBM=cmultRepl(DF.0U,method=\"GBM\",output=\"p-counts\",suppress.print=TRUE)\nbitxos.nounics=bitxos[-Unics]"
  },
  {
    "objectID": "coda_ejemplo1.html#imputaci√≥n-de-ceros-con-un-m√©todo-iterativo",
    "href": "coda_ejemplo1.html#imputaci√≥n-de-ceros-con-un-m√©todo-iterativo",
    "title": "4¬† CODA en acci√≥n üòé:",
    "section": "4.7 Imputaci√≥n de ceros con un m√©todo Iterativo",
    "text": "4.7 Imputaci√≥n de ceros con un m√©todo Iterativo\nEl m√©todo EM se basa en la idea de maximizar una funci√≥n de verosimilitud incompleta, donde se asume que los datos faltantes son valores no observados o ‚Äúdatos ocultos‚Äù.\n\n# impRZilr de la librer√≠a robCompositions\n# \n# Tarda mucho ... \n# DF.0n=as.data.frame(DF.0)\n# DF.0n=as.data.frame(apply(DF.0n,MARGIN=2,as.numeric))\n# DF.It=impRZilr(DF.0n, eps=0.05, method = \"pls\", dl=rep(1, dim(DF.0)[2]),maxit = 10,verbose = FALSE)\n# saveRDS(DF.It, file=\"DFItnou.RData\")\n\n\nDF.It=readRDS(\"DFItnou.RData\")$x"
  },
  {
    "objectID": "coda_ejemplo1.html#qu√©-m√©todo-para-imputar-los-ceros-es-mejor",
    "href": "coda_ejemplo1.html#qu√©-m√©todo-para-imputar-los-ceros-es-mejor",
    "title": "4¬† CODA en acci√≥n üòé:",
    "section": "4.8 ¬øQu√© m√©todo para imputar los ceros es mejor?",
    "text": "4.8 ¬øQu√© m√©todo para imputar los ceros es mejor?\n(Lubbe-Filznoser-Templ Chemolab 2021)\nComparando matrices de correlaciones de Kynclova-Hron-Filzmoser: un valor peque√±o indica que el m√©todo es mejor.\nLa funci√≥n corCoDa del paquete robCompositions que las calcula no aguanta matrices grandes (al menos en el port√°til), por lo tanto lo hacemos por muestreo.\n\n# Sustituci√≥n de los ceros por algo muy peque√±o\n# multRepl de la librer√≠a zCompositions\nDF.0.alt=multRepl(DF.0,dl=rep(1, ncol(DF.0)),frac=10^(-12),label=0)\nDF.0.alt.U=DF.0.alt[,-Unics]\n\n\n# X Y con las mismas dimensiones\n# m < 30 o  da NaN\nf=function(X,Y,m){\nx=sample(dim(X)[2],m)\n(1/m)^2*sum((corCoDa(X[,x])-corCoDa(Y[,x]))^2)\n}\nmean(replicate(200,f(DF.0.alt,DF.J,25)))\nmean(replicate(200,f(DF.0.alt.U,DF.GBM,25)))\nmean(replicate(200,f(DF.0.alt,DF.It,25)))\n# [1] 0.01688433 0.01664001\n# [1] 0.009403911 0.009399878\n# [1] 0.06253841 0.05720421"
  },
  {
    "objectID": "coda_ejemplo1.html#comparando-matrices-de-distancias-de-aitchison-valor-peque√±o-indica-mejor.",
    "href": "coda_ejemplo1.html#comparando-matrices-de-distancias-de-aitchison-valor-peque√±o-indica-mejor.",
    "title": "4¬† CODA en acci√≥n üòé:",
    "section": "4.9 Comparando matrices de distancias de Aitchison: valor peque√±o indica mejor.",
    "text": "4.9 Comparando matrices de distancias de Aitchison: valor peque√±o indica mejor.\n\n(1/dim(DF.0)[1])^2*sum((aDist(DF.0.alt)-aDist(DF.J))^2)\n\n[1] 20279.85\n\n(1/dim(DF.0U)[1])^2*sum((aDist(DF.0.alt.U)-aDist(DF.GBM))^2)\n\n[1] 19972.44\n\n(1/dim(DF.0)[1])^2*sum((aDist(DF.0.alt)-aDist(DF.It))^2)\n\n[1] 11971.71\n\n\n\nImputa0=c(\"J\",\"GBM\",\"It\")\nImputa0=\"GBM\"\nif (Imputa0==\"J\"){\n    DF=DF.J\n}\nif (Imputa0==\"GBM\"){\n  DF.0=DF.0U\n  DF=DF.GBM\n  bitxos=bitxos.nounics\n} \nif (Imputa0==\"It\"){\n    DF=DF.It\n}"
  },
  {
    "objectID": "coda_ejemplo1.html#si-se-quieren-filtrar-las-muestras-outliers",
    "href": "coda_ejemplo1.html#si-se-quieren-filtrar-las-muestras-outliers",
    "title": "4¬† CODA en acci√≥n üòé:",
    "section": "4.10 Si se quieren filtrar las muestras outliers",
    "text": "4.10 Si se quieren filtrar las muestras outliers\nQuitamos muestras que contribuyen a tener mucha varianza\nUtiliz√≥ codaSeq.outlier de funcionsCODACesc.R adaptada de EasyCODA, que le da error\nLa funci√≥n codaSeq.outlier ayuda a identificar observaciones que se desv√≠an significativamente de la estructura t√≠pica de las composiciones.\nEl algoritmo de detecci√≥n de outliers implementado en codaSeq.outlier se basa en la suposici√≥n de que los datos de composici√≥n siguen una distribuci√≥n log-ratio multivariada.\n\nDF.CLR=acomp(DF)\nCSOut=codaSeq.outlier(DF.CLR, plot.me=TRUE)\n\n\n\n\n\n\n\noutliers=CSOut$bad\nbones=CSOut$good\n\nLas muestras outliers son\n\nmostres[outliers]\n\n [1] \"A04T4\" \"A05T1\" \"A07T2\" \"A08T4\" \"A08T6\" \"E01T1\" \"E01T2\" \"E06T4\" \"E09T2\"\n[10] \"E10T3\" \"I01T2\" \"I01T4\" \"I02T7\" \"I05T2\" \"I07T4\" \"I07T6\" \"I07T7\" \"I09T2\"\n[19] \"I10T6\"\n\n\nSi quitamos estas muestras, tenemos que volver a controlar que no nos quede ninguna columna de 0s\n\nDF.0B=DF.0[bones,]\nconserv=which(apply(DF.0B, 2, sum)>0) \nDF.0B=DF.0B[ ,conserv]\n\n\n# si se emplea DF.It, igual conviene re-calcularlo porque depende de las muestras\n\n# Hay que quitar bichos que hayan quedado a 0 en todo\nconserv=which(apply(DF.0B, 2, sum)>0) \nDF.0B=DF.0B[ ,conserv]\nDF.0Bn=as.data.frame(DF.0B)\nDF.0Bn=as.data.frame(apply(DF.0Bn,MARGIN=2,as.numeric))\nDFB.It=impRZilr(DF.0Bn, eps=0.05, method = \"pls\", dl=rep(1, dim(DF.0B)[2]),maxit = 10,verbose = FALSE)\nsaveRDS(DFB.It, file=\"DFItBnou.RData\")\n\nCon la funci√≥n QuinesMostres indicamos si cogemos solo las muestras buenas o todas\n\n#QuinesMostres=c(\"totes\",\"bones\") \nQuinesMostres=\"bones\"\nif (QuinesMostres==\"bones\"){\n  DF.0=DF.0B\n  DF=DF[bones,conserv]\n  Grups=Grups[bones]\ncolors=colors[bones]\nmostres=mostres[bones]\n    bitxos=bitxos[conserv]\n}\nDF.CLR=acomp(DF)\nDF.prop=t(apply(DF, 1, function(x){x/sum(x)}))"
  },
  {
    "objectID": "coda_ejemplo1.html#sin-filtrar-variables",
    "href": "coda_ejemplo1.html#sin-filtrar-variables",
    "title": "4¬† CODA en acci√≥n üòé:",
    "section": "4.11 Sin filtrar variables",
    "text": "4.11 Sin filtrar variables\nNo hacemos biplot porque con tanta variable no se ve nada\n\n4.11.1 Clustering jer√°rquico\nA diferencia del algoritmo de Ward est√°ndar (agrupa observaciones similares en funci√≥n de una medida de distancia entre ellas), donde todas las observaciones tienen el mismo peso, el enfoque ponderado permite considerar la importancia relativa de cada observaci√≥n en el proceso de agrupamiento.\n\n# Clustering jer√°rquico con distancias euclidianas con pesos\n# Funci√≥n WARD de EasyCODA, necesita que la matriz de clr's se calcule con la misma librer√≠a\nDF.CLR.W=CLR(DF)\nhc=WARD(DF.CLR.W,weight=TRUE)\ndend=as.dendrogram(hc)\nlabels_colors(dend)=colors[hc$order]\npar(cex=0.75)\nplot(dend, main = \"\")\n\n\n\n\n\n\n\npar(cex=1)\n# No dibujamos barplot de composiciones porque el gr√°fico no es informativo\n\nMatriz para entender los resultados:\n\nclust1=data.frame(Orig=Grups[hc$order],\n             clust=cutree(dend, k = 3)[order.dendrogram(dend)])\ntable(clust1)%>%\n  kbl() %>%\n  kable_styling()\n\n\n\n \n  \n      \n    1 \n    2 \n    3 \n  \n \n\n  \n    A \n    51 \n    14 \n    0 \n  \n  \n    E \n    25 \n    28 \n    16 \n  \n  \n    I \n    57 \n    0 \n    8"
  },
  {
    "objectID": "coda_ejemplo1.html#aldex",
    "href": "coda_ejemplo1.html#aldex",
    "title": "4¬† CODA en acci√≥n üòé:",
    "section": "4.12 ALDEx",
    "text": "4.12 ALDEx\nPermite identificar caracter√≠sticas (como especies microbianas o genes) que muestran diferencias significativas entre grupos de muestras (A, E, I) utilizando un enfoque basado en la inferencia bayesiana. Las muestras se extraen de la distribuci√≥n Dirichlet (la beta en el caso multivariado).\n\nDF0.Aldex=rbind(DF.0[Grups==\"A\",],DF.0[Grups==\"E\",], DF.0[Grups==\"I\",])\nDF0.t=data.frame(t(DF0.Aldex))\nconds=c(rep(\"A\", dim(DF.0[Grups==\"A\",])[1]),rep(\"E\", dim(DF.0[Grups==\"E\",])[1]), rep(\"I\", dim(DF.0[Grups==\"I\",])[1]))\n#'\nx.clr.kw=aldex.clr(DF0.t[,1:5], conds=conds[1:5], mc.samples=10, verbose=FALSE)\nmc.instances <- numMCInstances(x.clr.kw)\nmc.all <- getMonteCarloInstances(x.clr.kw)\n\n\nDF0.Aldex=rbind(DF.0[Grups==\"A\",],DF.0[Grups==\"E\",], DF.0[Grups==\"I\",])\nDF0.t=data.frame(t(DF0.Aldex))\nconds=c(rep(\"A\", dim(DF.0[Grups==\"A\",])[1]),rep(\"E\", dim(DF.0[Grups==\"E\",])[1]), rep(\"I\", dim(DF.0[Grups==\"I\",])[1]))\n#\nx.clr.kw=aldex.clr(DF0.t, conds=conds, mc.samples=1000, verbose=FALSE)\nx.kw=aldex.kw(x.clr.kw, verbose=FALSE)\n# valores esperados del test Kruskal-Wallis y un glm sobre los datos\nmm=model.matrix(~conds,data.frame(conds))\nx.clr.glm=aldex.clr(DF0.t, conds=mm, mc.samples=1000, verbose=FALSE)\nx.glm=aldex.glm(x.clr.glm, mm)\n#\nx.tot=cbind(bitxos,x.kw,x.glm) \nsaveRDS(x.tot, file=\"xtotTotal.RData\")\n\n\nx.tot=readRDS(\"xtotTotal.RData\")\nx.tot$bitxos=paste0(\"OTU.\",c(paste(\"00\",1:9),paste(\"0\",10:99),100:276))\n\n\nx.tot=readRDS(\"xtotTotal.RData\")\n\n\nhead(x.tot)%>%\n  kbl() %>%\n  kable_styling()\n\n\n\n \n  \n    bitxos \n    kw.ep \n    kw.eBH \n    glm.ep \n    glm.eBH \n    Intercept::Est \n    Intercept::SE \n    Intercept::t.val \n    Intercept::pval \n    condsE:Est \n    condsE:SE \n    condsE:t.val \n    condsE:pval \n    condsI:Est \n    condsI:SE \n    condsI:t.val \n    condsI:pval \n    Intercept::pval.holm \n    condsE:pval.holm \n    condsI:pval.holm \n  \n \n\n  \n    OTU.00 1 \n    0.0801547 \n    0.1570136 \n    0.0303225 \n    0.0797589 \n    11.554637 \n    0.1854587 \n    62.30798 \n    0 \n    0.3413483 \n    0.2584491 \n    1.3206900 \n    0.1903230 \n    0.6980545 \n    0.2622782 \n    2.6615681 \n    0.0087659 \n    0 \n    1.0000000 \n    0.9977548 \n  \n  \n    OTU.00 2 \n    0.0206440 \n    0.0579031 \n    0.8077564 \n    0.8585892 \n    10.371632 \n    0.3250578 \n    31.95125 \n    0 \n    -0.2775871 \n    0.4529898 \n    -0.6145031 \n    0.5414810 \n    -0.1943723 \n    0.4597011 \n    -0.4203944 \n    0.6775020 \n    0 \n    1.0000000 \n    1.0000000 \n  \n  \n    OTU.00 3 \n    0.0000000 \n    0.0000001 \n    0.0000000 \n    0.0000001 \n    11.002260 \n    0.2578001 \n    42.77776 \n    0 \n    -1.3076398 \n    0.3592617 \n    -3.6427641 \n    0.0003698 \n    1.0649519 \n    0.3645844 \n    2.9277117 \n    0.0042881 \n    0 \n    0.0972514 \n    0.8239804 \n  \n  \n    OTU.00 4 \n    0.0005343 \n    0.0031584 \n    0.0136801 \n    0.0424850 \n    8.736565 \n    0.5371141 \n    16.30443 \n    0 \n    -2.1272875 \n    0.7485045 \n    -2.8451954 \n    0.0060917 \n    -0.4423784 \n    0.7595941 \n    -0.5828832 \n    0.5672890 \n    0 \n    0.8664487 \n    1.0000000 \n  \n  \n    OTU.00 5 \n    0.0017423 \n    0.0078298 \n    0.0006925 \n    0.0041580 \n    12.369761 \n    0.1919890 \n    64.43325 \n    0 \n    -0.8996720 \n    0.2675496 \n    -3.3628596 \n    0.0009802 \n    -0.0119042 \n    0.2715135 \n    -0.0438439 \n    0.9315986 \n    0 \n    0.2511886 \n    1.0000000 \n  \n  \n    OTU.00 6 \n    0.0008415 \n    0.0044561 \n    0.0350900 \n    0.0835593 \n    9.693963 \n    0.2458796 \n    39.53140 \n    0 \n    0.0115472 \n    0.3426497 \n    0.0338475 \n    0.9425424 \n    0.8127551 \n    0.3477263 \n    2.3506040 \n    0.0246736 \n    0 \n    1.0000000 \n    1.0000000 \n  \n\n\n\n\n\nAqu√≠ podemos basarnos en:\n\nkw.eBH: ajuste de p-valores utilizando el m√©todo de Benjamini-Hochberg (BH) despu√©s de realizar un an√°lisis de varianza de Kruskal-Wallis (KW).\nglm.eBH: ajuste de p-valores utilizando el m√©todo de Benjamini-Hochberg (BH) despu√©s de ajustar un modelo lineal generalizado (GLM)\nmodel.condsE: se refiere al modelo de regresi√≥n ajustado utilizando las variables de composici√≥n como predictores y la condici√≥n ‚ÄúcondsE‚Äù como la variable de respuesta.\n\n\nsignif.p1=which(x.tot$kw.eBH < 0.01)\nsignif.g1=which(x.tot$glm.eBH < 0.01)\nsignif.p1=intersect(signif.p1,signif.g1)\n\nlength(signif.p1)\n\n[1] 40\n\nx.tot.sign=x.tot[signif.p1,c(3,5,19,20)]\nnames(x.tot.sign)=c(\"p-val KW corregit\", \"p-val glm corregit\", \"p-val E vs A corregit\", \"p-val I vs A corregit\")\nrownames(x.tot.sign)=bitxos[signif.p1]\n\nx.tot.sign%>%\n  kbl() %>%\n  kable_styling()\n\n\n\n \n  \n      \n    p-val KW corregit \n    p-val glm corregit \n    p-val E vs A corregit \n    p-val I vs A corregit \n  \n \n\n  \n    OTU.003 \n    0.0000001 \n    0.0000001 \n    0.0972514 \n    0.8239804 \n  \n  \n    OTU.005 \n    0.0078298 \n    0.0041580 \n    0.2511886 \n    1.0000000 \n  \n  \n    OTU.008 \n    0.0000070 \n    0.0000016 \n    1.0000000 \n    0.0000085 \n  \n  \n    OTU.011 \n    0.0010125 \n    0.0059162 \n    1.0000000 \n    0.0787355 \n  \n  \n    OTU.013 \n    0.0000433 \n    0.0000009 \n    0.2828991 \n    0.5834326 \n  \n  \n    OTU.016 \n    0.0038849 \n    0.0006962 \n    0.0327502 \n    1.0000000 \n  \n  \n    OTU.020 \n    0.0097656 \n    0.0033890 \n    1.0000000 \n    0.0918489 \n  \n  \n    OTU.022 \n    0.0000326 \n    0.0000097 \n    1.0000000 \n    0.0358061 \n  \n  \n    OTU.025 \n    0.0002163 \n    0.0001919 \n    1.0000000 \n    0.0091624 \n  \n  \n    OTU.030 \n    0.0000002 \n    0.0000000 \n    0.0002360 \n    0.0000002 \n  \n  \n    OTU.034 \n    0.0000000 \n    0.0000000 \n    1.0000000 \n    0.0000011 \n  \n  \n    OTU.038 \n    0.0001000 \n    0.0006898 \n    1.0000000 \n    0.0438187 \n  \n  \n    OTU.044 \n    0.0000015 \n    0.0000002 \n    1.0000000 \n    0.0000073 \n  \n  \n    OTU.046 \n    0.0011419 \n    0.0006191 \n    0.5639068 \n    0.0063995 \n  \n  \n    OTU.047 \n    0.0001250 \n    0.0001823 \n    0.2585800 \n    0.9991464 \n  \n  \n    OTU.049 \n    0.0053303 \n    0.0072328 \n    0.1026868 \n    0.9789205 \n  \n  \n    OTU.050 \n    0.0000264 \n    0.0002277 \n    0.3148470 \n    1.0000000 \n  \n  \n    OTU.052 \n    0.0001237 \n    0.0075423 \n    0.9952454 \n    1.0000000 \n  \n  \n    OTU.054 \n    0.0013229 \n    0.0097693 \n    1.0000000 \n    0.7449599 \n  \n  \n    OTU.057 \n    0.0000489 \n    0.0000655 \n    1.0000000 \n    0.1825672 \n  \n  \n    OTU.059 \n    0.0000000 \n    0.0000000 \n    1.0000000 \n    0.0000004 \n  \n  \n    OTU.060 \n    0.0002872 \n    0.0042465 \n    0.6414395 \n    1.0000000 \n  \n  \n    OTU.063 \n    0.0004161 \n    0.0014406 \n    0.4197127 \n    1.0000000 \n  \n  \n    OTU.064 \n    0.0000174 \n    0.0001876 \n    1.0000000 \n    0.0845730 \n  \n  \n    OTU.066 \n    0.0000025 \n    0.0000036 \n    0.0061348 \n    0.0000254 \n  \n  \n    OTU.067 \n    0.0003092 \n    0.0026454 \n    0.7734519 \n    0.0414771 \n  \n  \n    OTU.075 \n    0.0091920 \n    0.0052941 \n    0.1359825 \n    1.0000000 \n  \n  \n    OTU.077 \n    0.0000837 \n    0.0003245 \n    0.9095772 \n    0.9990152 \n  \n  \n    OTU.078 \n    0.0001277 \n    0.0005558 \n    0.7500225 \n    1.0000000 \n  \n  \n    OTU.086 \n    0.0005582 \n    0.0022955 \n    0.1265078 \n    1.0000000 \n  \n  \n    OTU.088 \n    0.0000000 \n    0.0000000 \n    1.0000000 \n    0.0000000 \n  \n  \n    OTU.091 \n    0.0000012 \n    0.0000026 \n    0.6333928 \n    0.4631056 \n  \n  \n    OTU.096 \n    0.0007613 \n    0.0024963 \n    0.7773785 \n    0.9987909 \n  \n  \n    OTU.100 \n    0.0087198 \n    0.0049639 \n    0.4523354 \n    1.0000000 \n  \n  \n    OTU.101 \n    0.0000000 \n    0.0000000 \n    1.0000000 \n    0.0000138 \n  \n  \n    OTU.102 \n    0.0000000 \n    0.0000053 \n    0.9922000 \n    0.0525332 \n  \n  \n    OTU.111 \n    0.0004223 \n    0.0016563 \n    0.3737696 \n    0.0432119 \n  \n  \n    OTU.113 \n    0.0021340 \n    0.0040426 \n    0.1122496 \n    1.0000000 \n  \n  \n    OTU.118 \n    0.0000004 \n    0.0000050 \n    0.8946424 \n    0.0804560 \n  \n  \n    OTU.154 \n    0.0010999 \n    0.0060971 \n    0.1554282 \n    0.9981464 \n  \n\n\n\n\nprova=unlist(sapply(rownames(x.tot.sign),FUN=function(x) which(x==x.tot$bitxos)))\nnames(prova)=NULL\n\n Si nos restringimos a estos bichos :\n\nDF.s=DF[,prova]\nDF.CLR=acomp(DF.s) \n\n\npcx=princomp(DF.CLR,cor=TRUE) \n\n\ncoloredBiplot(pcx, cex=0.5, col=\"red\",  arrow.len=0, scale=1,var.axes=TRUE,\n    xlab=paste(\"PC1\", round(pcx$sdev[1]^2 / sum(pcx$sdev^2),3), sep=\": \"),\n    ylab=paste(\"PC2\", round(pcx$sdev[2]^2 / sum(pcx$sdev^2),3), sep=\": \"),\n    xlabs.col=colors, main=\"Form biplot\")\n\n\n\n\n\n\n\n\n\nDF.CLR.W.s=CLR(DF.s)\nhc2=WARD(DF.CLR.W.s,weight=TRUE)\ndend.2=as.dendrogram(hc2)\nlabels_colors(dend.2)=colors[hc2$order]\nDFOr=DF.s[hc2$order,]\n#Reordemanos las muestras para dibujar los barplot en el mismo orden\nDFOr.CLR=acomp(DFOr)\nd.names=colnames(DF.s)[order(apply(DF.s, 2, sum), decreasing=T) ]\nnb.cols=dim(DF.s)[2]\ncolors.OTU=colorRampPalette(brewer.pal(length(d.names),\"Spectral\"))(nb.cols)\n#Dibujo\nlayout(matrix(c(1,3,2,3),2,2, byrow=T), widths=c(6,2), height=c(4,4))\npar(mar=c(2,1,1,1)+0.1,cex=0.75)\nplot(dend.2, main = \"\")\nbarplot(DFOr.CLR, legend.text=F, col=colors.OTU, axisnames=F, border=NA, xpd=T,)\npar(mar=c(0,1,1,1)+0.1,cex=1)\nplot(1,2, pch = 1, lty = 1, ylim=c(-20,20), type = \"n\", axes = FALSE, ann = FALSE)\nlegend(x=\"center\", legend=d.names, col=colors.OTU, lwd=5, cex=.6, border=NULL)"
  }
]